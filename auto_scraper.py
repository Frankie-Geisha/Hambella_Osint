import os
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import json
from supabase import create_client, Client

# ==========================================
# ğŸŒ¸ å¹½çµæš—å“¨ï¼šå…¨è‡ªåŠ¨åŒ– OSINT æŠ“å–å¼•æ“
# ==========================================

# 1. ä»äº‘ç«¯ç¯å¢ƒå˜é‡è·å–å¯†é’¥ï¼ˆè€Œä¸æ˜¯ Streamlit Secretsï¼‰
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")

if not all([SUPABASE_URL, SUPABASE_KEY, DEEPSEEK_API_KEY]):
    print("ğŸš¨ è‡´å‘½é”™è¯¯ï¼šç¯å¢ƒå˜é‡ç¼ºå¤±ï¼Œæ— äººæœºå¯åŠ¨å¤±è´¥ï¼")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 2. ç›®æ ‡é¢‘é“æ¸…å•ï¼ˆå¿…é¡»ä¸ä¸»å¤§å…ä¿æŒåŒæ­¥ï¼‰
channel_urls = [
    "https://t.me/s/ejdailyru","https://t.me/s/Ateobreaking", "https://t.me/s/theinsider", "https://t.me/s/moscowtimes_ru",
    "https://t.me/s/economica","https://t.me/s/rybar_africa","https://t.me/s/zakupki_time","https://t.me/s/truestorymedia",
    "https://t.me/s/AoMurmansk","https://t.me/s/moscow_laundry","https://t.me/s/svtvnews","https://t.me/s/notes_veterans",
    "https://t.me/s/militarysummary","https://t.me/s/Tolo_news","https://t.me/s/kremlin_sekret","https://t.me/s/dva_majors",
    "https://t.me/s/caucasar","https://t.me/s/rybar","https://t.me/s/olen_nn","https://t.me/s/russicaRU",
    "https://t.me/s/topwar_official","https://t.me/s/RusskajaIdea","https://t.me/s/riakatysha","https://t.me/s/rybar_latam",
    "https://t.me/s/zhivoff","https://t.me/s/anserenko","https://t.me/s/wolframiumZ","https://t.me/s/vatnoeboloto","https://t.me/s/romanromachev",
    "https://t.me/s/thehegemonist","https://t.me/s/budni_manipulyatora","https://t.me/s/ManoiloToday","https://t.me/s/rtechnocom",
    "https://t.me/s/darpaandcia","https://t.me/s/istories_media","https://t.me/s/mediazona_exclusive","https://t.me/s/Russian_OSINT",
    "https://t.me/s/alter_academy","https://t.me/s/rybar_mena","https://t.me/s/rybar_pacific","https://t.me/s/mosnews","https://t.me/s/brieflyru"
]
VIP_CHANNELS = ["anserenko", "kremlin_sekret","rybar","Russian_OSINT","rybar_mena","rybar_pacific","topwar_official"] 

def load_bookmarks():
    try:
        res = supabase.table("bookmarks_db").select("*").execute()
        return {row['channel_name']: row['last_read_id'] for row in res.data}
    except: return {}

def save_bookmarks(bookmarks):
    try:
        data = [{"channel_name": k, "last_read_id": v} for k, v in bookmarks.items()]
        if data: supabase.table("bookmarks_db").upsert(data).execute()
    except: pass

def run_auto_scrape():
    print("ğŸš æ— äººæœºå‡ç©ºï¼šå¼€å§‹æ‰§è¡Œé™é»˜æŠ“å–ä»»åŠ¡...")
    bookmarks = load_bookmarks()
    raw_intelligence = ""
    new_msg_count = 0
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    for url in channel_urls:
        try:
            channel_name = url.split('/s/')[-1]
            last_read_id = bookmarks.get(channel_name, 0)
            highest_id = last_read_id
            
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            message_blocks = soup.find_all('div', class_='tgme_widget_message')
            if last_read_id == 0: message_blocks = message_blocks[-5:] 
                
            channel_new_text = ""
            for block in message_blocks:
                post_id_str = block.get('data-post')
                text_div = block.find('div', class_='tgme_widget_message_text')
                time_tag = block.find('time')
                msg_time = time_tag.get('datetime', '')[:16].replace('T', ' ') if time_tag else "æœªçŸ¥æ—¶é—´"
                
                if post_id_str and text_div:
                    msg_id = int(post_id_str.split('/')[-1])
                    if msg_id > last_read_id:
                        channel_new_text += f"[å‘å¸–æ—¶é—´: {msg_time}] " + text_div.text + "\n"
                        new_msg_count += 1
                        if msg_id > highest_id: highest_id = msg_id
            
            if channel_new_text != "":
                is_vip = "ã€ğŸ”´ VIP å¿…é¡»æç‚¼ã€‘" if channel_name in VIP_CHANNELS else ""
                raw_intelligence += f"\n\n--- æ¥æºï¼š{channel_name} {is_vip} ---\n" + channel_new_text
                bookmarks[channel_name] = highest_id
        except Exception as e: 
            print(f"âš ï¸ é¢‘é“ {channel_name} æŠ“å–å¤±è´¥: {e}")
            
    save_bookmarks(bookmarks)
    
    if new_msg_count == 0:
        print("ğŸŸ¢ ä¾¦å¯Ÿå®Œæ¯•ï¼šæš‚æ— æ–°æƒ…æŠ¥ã€‚")
        return

    print(f"ğŸ”¥ æˆªè· {new_msg_count} æ¡åŸå§‹ä¿¡æ¯ï¼Œå‘¼å« DeepSeek å¼•æ“è¿›è¡Œæç‚¼...")
    
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")
    system_prompt = """
    ä½ æ˜¯ä¸€ä½é¡¶çº§çš„åœ°ç¼˜æ”¿æ²»ä¸ OSINT åˆ†æå®˜ã€‚
    è¯·åˆ†æåŸå§‹æ¶ˆæ¯ï¼Œæµ“ç¼©æˆç‹¬ç«‹æƒ…æŠ¥ï¼Œå¿…é¡»å½»åº•ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡ï¼
    âš ï¸ æå…¶é‡è¦æŒ‡ä»¤ 1ï¼šåŸå§‹æ–‡æœ¬ä¸­å¸¦æœ‰ [å‘å¸–æ—¶é—´: ...]ã€‚å¦‚æœæœ‰å¤šä¸ªæ¥æºè®²è¿°åŒä¸€ä»¶äº‹ï¼Œè¯·æå–å‡ºå…¶ä¸­æœ€æ—©çš„é‚£ä¸ªæ—¶é—´ï¼Œæ ¼å¼ä¸º YYYY-MM-DD HH:MMã€‚
    âš ï¸ æå…¶é‡è¦æŒ‡ä»¤ 2ï¼šå¦‚æœè¯¥æ¡ä¿¡æ¯çš„æ¥æºå¸¦æœ‰ "ã€ğŸ”´ VIP å¿…é¡»æç‚¼ã€‘" çš„æ ‡è®°ï¼Œè¯·åœ¨è¾“å‡ºçš„ "summary" å­—æ®µæœ€åï¼Œè¿½åŠ  "ã€ğŸ’ VIP åŸæ–‡å…¨è¯‘ã€‘ï¼š" åŠå®Œæ•´çš„ä¸­æ–‡ç¿»è¯‘ã€‚
    âš ï¸ æå…¶é‡è¦æŒ‡ä»¤ 3ï¼šè¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼åˆæ³•çš„ JSONï¼JSON å†…éƒ¨æ¢è¡Œå¿…é¡»ç”¨ "\\n"ï¼ŒåŒå¼•å·å¿…é¡»ç”¨ "\\" è½¬ä¹‰ï¼
    
    ã€ğŸ¯ æ ¸å¿ƒæˆ˜æœ¯æ‰“åˆ†é‡è¡¨ (score: 0-100)ã€‘ï¼š
    - 90-100åˆ† (æé«˜å±/æˆ˜ç•¥çº§)ï¼šå°†æ”¹å˜åœ°ç¼˜æ ¼å±€ã€é‡å¤§é«˜å±‚æ¸…æ´—/äººäº‹çªå˜ã€æ¶‰åé‡å¤§è´Ÿé¢/æ ¸å¿ƒåˆ©ç›Šé“¾å¼‚åŠ¨ã€‚
    - 70-89åˆ† (é«˜ä»·å€¼çº¿ç´¢)ï¼šä¸­ç­‰è§„æ¨¡çªå‘å†²çªã€å…³é”®ä¾›åº”é“¾/èƒ½æºç½‘å¼‚åŠ¨ã€‚
    - 40-69åˆ† (ä¸€èˆ¬æƒ…æŠ¥)ï¼šå¸¸è§„æˆ˜å†µæ’­æŠ¥ã€ä¾‹è¡Œå¤–äº¤è¾ä»¤ã€å®è§‚ç»æµæ•°æ®çš„ä¸€èˆ¬æ³¢åŠ¨ã€‚
    - 0-39åˆ† (ä¿¡æ¯å™ªç‚¹)ï¼šæ— æ„ä¹‰å®£ä¼ ã€æœªç»è¯å®çš„è¾¹ç¼˜å…«å¦ã€‚ï¼ˆå°½é‡å‰”é™¤æ­¤ç±»ä¿¡æ¯ï¼‰ã€‚
    
    ã€è¾“å‡ºåˆæ³• JSONã€‘ï¼š
    {
        "reports": [
            {"title": "ä¸­æ–‡æ ‡é¢˜", "summary": "ä¸­æ–‡æ¦‚è¿°åŠVIPå…¨æ–‡", "category": "China Nexus ç­‰ä»£å·", "score": 85, "source": "é¢‘é“", "publish_time": "æœ€æ—©å‘å¸ƒæ—¶é—´(YYYY-MM-DD HH:MM)"}
        ]
    }
    """
    try:
        ai_response = client.chat.completions.create(
            model="deepseek-chat", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": raw_intelligence}],
            response_format={"type": "json_object"}, max_tokens=8000
        )
        reports = json.loads(ai_response.choices[0].message.content).get("reports", [])
        for rep in reports:
            supabase.table("intelligence_db").insert({
                "title": rep.get("title", "æ— æ ‡é¢˜"), "summary": rep.get("summary", "æ— å†…å®¹"),
                "category": rep.get("category", "Global Macro"), "score": rep.get("score", 0), 
                "source": rep.get("source", "æœªçŸ¥"), "publish_time": rep.get("publish_time", "æœªçŸ¥æ—¶é—´")
            }).execute()
        print(f"âœ… ä»»åŠ¡åœ†æ»¡å®Œæˆï¼å·²å‘ Supabase æ•°æ®åº“æˆåŠŸå­˜å…¥ {len(reports)} æ¡ä¸­æ–‡é«˜ä»·å€¼æƒ…æŠ¥ï¼")
    except Exception as e:
        print(f"ğŸš¨ DeepSeek è§£ææˆ–å†™å…¥æ•°æ®åº“å¤±è´¥ï¼š{e}")

if __name__ == "__main__":
    run_auto_scrape()

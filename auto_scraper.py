import os
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import json
from supabase import create_client, Client

# ==========================================
# ğŸŒ¸ å¹½çµæš—å“¨ï¼šå…¨è‡ªåŠ¨åŒ– OSINT æŠ“å–å¼•æ“
# ==========================================

# 1. ä»äº‘ç«¯ç¯å¢ƒå˜é‡è·å–å¯†é’¥ï¼ˆè€Œä¸æ˜¯ Streamlit Secretsï¼‰
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")

if not all([SUPABASE_URL, SUPABASE_KEY, DEEPSEEK_API_KEY]):
    print("ğŸš¨ è‡´å‘½é”™è¯¯ï¼šç¯å¢ƒå˜é‡ç¼ºå¤±ï¼Œæ— äººæœºå¯åŠ¨å¤±è´¥ï¼")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 2. ç›®æ ‡é¢‘é“æ¸…å•ï¼ˆå¿…é¡»ä¸ä¸»å¤§å…ä¿æŒåŒæ­¥ï¼‰
channel_urls = [
    "https://t.me/s/ejdailyru","https://t.me/s/Ateobreaking", "https://t.me/s/theinsider", "https://t.me/s/moscowtimes_ru",
    "https://t.me/s/economica","https://t.me/s/rybar_africa","https://t.me/s/zakupki_time","https://t.me/s/truestorymedia",
    "https://t.me/s/AoMurmansk","https://t.me/s/moscow_laundry","https://t.me/s/svtvnews","https://t.me/s/notes_veterans",
    "https://t.me/s/militarysummary","https://t.me/s/Tolo_news","https://t.me/s/kremlin_sekret","https://t.me/s/dva_majors",
    "https://t.me/s/caucasar","https://t.me/s/rybar","https://t.me/s/olen_nn","https://t.me/s/russicaRU",
    "https://t.me/s/topwar_official","https://t.me/s/RusskajaIdea","https://t.me/s/riakatysha","https://t.me/s/rybar_latam",
    "https://t.me/s/zhivoff","https://t.me/s/anserenko","https://t.me/s/wolframiumZ","https://t.me/s/vatnoeboloto","https://t.me/s/romanromachev",
    "https://t.me/s/thehegemonist","https://t.me/s/budni_manipulyatora","https://t.me/s/ManoiloToday","https://t.me/s/rtechnocom",
    "https://t.me/s/darpaandcia","https://t.me/s/istories_media","https://t.me/s/mediazona_exclusive","https://t.me/s/Russian_OSINT",
    "https://t.me/s/alter_academy","https://t.me/s/rybar_mena","https://t.me/s/rybar_pacific","https://t.me/s/mosnews","https://t.me/s/brieflyru"
]
VIP_CHANNELS = ["anserenko", "kremlin_sekret","rybar","Russian_OSINT","rybar_mena","rybar_pacific","topwar_official"] 

def load_bookmarks():
    try:
        res = supabase.table("bookmarks_db").select("*").execute()
        return {row['channel_name']: row['last_read_id'] for row in res.data}
    except: return {}

def save_bookmarks(bookmarks):
    try:
        data = [{"channel_name": k, "last_read_id": v} for k, v in bookmarks.items()]
        if data: supabase.table("bookmarks_db").upsert(data).execute()
    except: pass

def run_auto_scrape():
    print("ğŸš æ— äººæœºå‡ç©ºï¼šå¼€å§‹æ‰§è¡Œç½‘æ ¼åŒ–é™é»˜æŠ“å–ä»»åŠ¡...")
    bookmarks = load_bookmarks()
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    # ğŸŒŸ æˆ˜æœ¯å‡çº§ 1ï¼šåˆ†é˜Ÿç½‘æ ¼åŒ–æœç´¢ï¼ˆæ¯ 8 ä¸ªé¢‘é“ä¸ºä¸€ç»„ï¼Œé˜²æ­¢ AI æ³¨æ„åŠ›æ¶£æ•£ï¼‰
    BATCH_SIZE = 8
    channel_batches = [channel_urls[i:i + BATCH_SIZE] for i in range(0, len(channel_urls), BATCH_SIZE)]
    
    total_saved = 0
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")
    
    for batch_index, batch_urls in enumerate(channel_batches):
        raw_intelligence = ""
        new_msg_count = 0
        
        for url in batch_urls:
            try:
                channel_name = url.split('/s/')[-1]
                last_read_id = bookmarks.get(channel_name, 0)
                highest_id = last_read_id
                
                channel_new_text = ""
                current_url = url
                pages_fetched = 0
                
                # ğŸŒŸ V5.2 æ—¶å…‰å€’æµå¼•æ“ï¼šæœ€å¤šå¾€å‰ç¿»é˜… 5 é¡µï¼ˆçº¦ 100 æ¡å†å²æ¶ˆæ¯ï¼‰ï¼Œé˜²æ­¢æ­»å¾ªç¯
                while pages_fetched < 5:
                    response = requests.get(current_url, headers=headers)
                    soup = BeautifulSoup(response.text, 'html.parser')
                    message_blocks = soup.find_all('div', class_='tgme_widget_message')
                    
                    if not message_blocks: break
                    
                    # ç¬¬ä¸€æ¬¡æ”¶å½•è¯¥é¢‘é“æ—¶ï¼Œé™åˆ¶æŠ“å–æœ€æ–° 20 æ¡ï¼Œä¸ç¿»é¡µ
                    if last_read_id == 0 and pages_fetched == 0: 
                        message_blocks = message_blocks[-20:] 
                        
                    page_has_new_msg = False
                    oldest_msg_id_on_page = 999999999
                    page_text = ""
                    
                    for block in message_blocks:
                        post_id_str = block.get('data-post')
                        text_div = block.find('div', class_='tgme_widget_message_text')
                        time_tag = block.find('time')
                        msg_time = time_tag.get('datetime', '')[:16].replace('T', ' ') if time_tag else "æœªçŸ¥æ—¶é—´"
                        
                        if post_id_str and text_div:
                            msg_id = int(post_id_str.split('/')[-1])
                            if msg_id < oldest_msg_id_on_page: 
                                oldest_msg_id_on_page = msg_id
                            
                            if msg_id > last_read_id:
                                page_text += f"[å‘å¸–æ—¶é—´: {msg_time}] " + text_div.text + "\n"
                                new_msg_count += 1
                                page_has_new_msg = True
                                if msg_id > highest_id: highest_id = msg_id
                    
                    # å·§å¦™æ‹¼æ¥ï¼šæŠŠæ—§ç½‘é¡µæå‡ºæ¥çš„ä¿¡æ¯ï¼Œå«åœ¨å‰é¢ï¼Œä¿è¯æœ€ç»ˆ AI é˜…è¯»çš„æ—¶é—´çº¿æ˜¯é¡ºç•…çš„ï¼
                    channel_new_text = page_text + channel_new_text
                    
                    # ğŸ’¡ æ ¸å¿ƒæœºå…³ï¼šå¦‚æœè¿™ä¸€é¡µæœ€è€çš„æ¶ˆæ¯ä¾ç„¶æ¯”ä¹¦ç­¾æ–°ï¼Œä¸”è¿™ä¸æ˜¯ä¸ªæ–°é¢‘é“ï¼Œè§¦å‘ç¿»é¡µï¼
                    if oldest_msg_id_on_page > last_read_id and page_has_new_msg and last_read_id != 0:
                        current_url = f"{url}?before={oldest_msg_id_on_page}"
                        pages_fetched += 1
                    else:
                        break # æ— ç¼è¡”æ¥æˆåŠŸï¼Œé€€å‡ºç¿»é¡µå¾ªç¯
                        
                if channel_new_text != "":
                    is_vip = "ã€ğŸ”´ VIP å¿…é¡»æç‚¼ã€‘" if channel_name in VIP_CHANNELS else ""
                    raw_intelligence += f"\n\n--- æ¥æºï¼š{channel_name} {is_vip} ---\n" + channel_new_text
                    bookmarks[channel_name] = highest_id
            except Exception as e: 
                print(f"âš ï¸ é¢‘é“ {url} æŠ“å–å¼‚å¸¸: {e}")
                
        # å­˜ä¸‹ä¹¦ç­¾
        save_bookmarks(bookmarks)
        
        # ğŸŒŸ æˆ˜æœ¯å‡çº§ 3ï¼šå°é˜Ÿç‹¬ç«‹æ±‡æŠ¥åˆ¶
        if new_msg_count > 0:
            print(f"ğŸ”¥ ç¬¬ {batch_index + 1} å°é˜Ÿæˆªè· {new_msg_count} æ¡ä¿¡æ¯ï¼Œå‘¼å« AI æç‚¼...")
            system_prompt = """
            ä½ æ˜¯ä¸€ä½é¡¶çº§çš„åœ°ç¼˜æ”¿æ²»ä¸ OSINT åˆ†æå®˜ã€‚
            è¯·åˆ†æåŸå§‹æ¶ˆæ¯ï¼Œæµ“ç¼©æˆç‹¬ç«‹æƒ…æŠ¥ï¼Œå¿…é¡»å½»åº•ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡ï¼
            âš ï¸ æå…¶é‡è¦æŒ‡ä»¤ï¼šç”±äºä½ ç°åœ¨å¤„ç†çš„æ˜¯ç²¾ç»†åŒ–å°æ‰¹æ¬¡æ•°æ®ï¼Œè¯·åŠ¡å¿…â€œå®æ»¥å‹¿ç¼ºâ€ï¼åªè¦åŒ…å«å…·ä½“äº‹ä»¶ã€è§‚ç‚¹ã€åŠ¨å‘ï¼Œå³ä¾¿ä½ è®¤ä¸ºä»·å€¼ä¸é«˜ï¼Œä¹Ÿè¦æå–å‡ºæ¥ï¼ˆå¯ä»¥æ‰“ä½åˆ†ï¼‰ï¼Œç»ä¸èƒ½éšæ„ä¸¢å¼ƒåŸå§‹ä¿¡æ¯ï¼
            âš ï¸ VIP é¢‘é“å¿…é¡»åœ¨ summary ç»“å°¾è¿½åŠ ã€ğŸ’ VIP åŸæ–‡å…¨è¯‘ã€‘ï¼šã€‚
            âš ï¸ è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼åˆæ³•çš„ JSONï¼JSON å†…éƒ¨æ¢è¡Œå¿…é¡»ç”¨ "\\n"ï¼ŒåŒå¼•å·å¿…é¡»ç”¨ "\\" è½¬ä¹‰ï¼
            
            ã€è¾“å‡ºåˆæ³• JSONã€‘ï¼š
            {
                "reports": [
                    {"title": "ä¸­æ–‡æ ‡é¢˜", "summary": "ä¸­æ–‡æ¦‚è¿°", "category": "China Nexus æˆ–å…¶ä»–", "score": 85, "source": "é¢‘é“", "publish_time": "YYYY-MM-DD HH:MM"}
                ]
            }
            """
            try:
                ai_response = client.chat.completions.create(
                    model="deepseek-chat", messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": raw_intelligence}],
                    response_format={"type": "json_object"}, max_tokens=8000
                )
                reports = json.loads(ai_response.choices[0].message.content).get("reports", [])
                for rep in reports:
                    supabase.table("intelligence_db").insert({
                        "title": rep.get("title", "æ— æ ‡é¢˜"), "summary": rep.get("summary", "æ— å†…å®¹"),
                        "category": rep.get("category", "Global Macro"), "score": rep.get("score", 0), 
                        "source": rep.get("source", "æœªçŸ¥"), "publish_time": rep.get("publish_time", "æœªçŸ¥æ—¶é—´")
                    }).execute()
                total_saved += len(reports)
            except Exception as e:
                print(f"ğŸš¨ ç¬¬ {batch_index + 1} å°é˜Ÿ DeepSeek è§£æå¤±è´¥ï¼š{e}")
        else:
            print(f"ğŸŸ¢ ç¬¬ {batch_index + 1} å°é˜Ÿæš‚æ— æ–°æƒ…æŠ¥ã€‚")

    print(f"âœ… å…¨ç½‘æ ¼æ‰«è¡å®Œæ¯•ï¼å…±å‘ Supabase å­˜å…¥ {total_saved} æ¡é«˜ä»·å€¼æƒ…æŠ¥ï¼")

if __name__ == "__main__":
    run_auto_scrape()
